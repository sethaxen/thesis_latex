\documentclass[../../main.tex]{subfiles}

\begin{document}

\chapter{Automatic differentation rules for power series functions of diagonalizable matrices}\label{ad-power-series}
\chaptermark {Differentating power series functions}

\begin{refsection}

	\section{Introduction}\label{introduction-1}

	We consider here power series functions of diagonalizable real and complex matrices.
	Given a function $f \colon \bbC \to \bbC$ that can be written as a power series of input $x$, that is,
	\[f(x) = \sum_{k=0}^\infty a_k x^k,\]
	we define its extension $F$ to complex square matrices $X \in \bbC^{n \times n}$ with the analogous matrix power series
	\[F(X) = \sum_{k=0}^\infty a_k X^k.\]
	Examples of $f$ and $F$ pairs are the scalar and matrix exponential, logarithmic, trigonometric, and hyperbolic functions.
	The matrix exponential is our motivating example.

	That $f$ and $F$ are closely related can be seen if we restrict $X$ to the diagonalizable matrices
	\[X = V \Lambda V^{-1},\]
	where $V$ is an invertible matrix of eigenvectors, and
	$\Lambda = \mqty(\dmat{\lambda_1, \ddots, \lambda_n})$
	is a diagonal matrix of eigenvalues $\lambda \in \bbC^{n}$.
	Powers of diagonalizable matrices can be computed as follows:
	$$X^k = (V \Lambda V^{-1})^k = V \Lambda^k V^{-1} = V \mqty(\dmat{\lambda_1^k, \ddots, \lambda_n^k}) V^{-1}.$$
	Consequently, the power series likewise simplifies:
	\[F(X) = \sum_{k=0}^\infty a_k V \Lambda^k V^{-1} = V \qty( \sum_{k=0}^\infty a_k \Lambda^k ) V^{-1} = V F(\Lambda) V^{-1} = V \mqty(\dmat{f(\lambda_1), \ddots, f(\lambda_n)}) V^{-1}.\]

	That is, for diagonalizable matrices, the function $F$ of the matrix can be computed by applying the function $f$ with the same power series to each eigenvalue of $X$.
	Because all such function pairs $f$ and $F$ share this connection for the diagonalizable matrices, their forward- and reverse-mode automatic differentiation rules share a common form.

	\clearpage % orphaned header
	\section{Forward-mode rule}\label{forward-mode-rule-1}

	For maximum generality, we consider a modification of the functions $f$ and $F$, namely $f(x, \theta)$ and $F(X, \theta)$ for some scalar $\theta$.
	We break the computation of $F(X, \theta)$ into several steps:
	\begin{align*}
		(\lambda, V) & = \operatorname{eigen}(X) \\
		\mu_i        & = f(\lambda_i, \theta)    \\
		M            & = \diag(\mu)              \\
		\Omega       & = V M V^{-1}.
	\end{align*}
	\cite[Section 3.1]{gilesExtendedCollectionMatrix2008} gives the directional derivative of the eigendecomposition as
	\begin{align*}
		\diag(\Dtan{\lambda}) & = I \circ (V^{-1} \Dtan{X} V)                              \\
		\Dtan{V}              & = V \left(W \circ \left(V^{-1} \Dtan{X} V \right) \right),
	\end{align*}
	where $\circ$ is the Hadamard (elementwise) product, $I$ is the identity matrix, and $W$ is the matrix with entries
	\begin{align*}
		W_{ij} = \begin{cases}
			(\lambda_j - \lambda_i)^{-1} \quad & \text{if } i \ne j \\
			0 \quad                            & \text{if } i = j
		\end{cases}.
	\end{align*}
	Note that $W$ is skew-symmetric, that is, $W = -\trans{W}$.
	$\Dtan{\mu}$ depends on the partial derivatives of $f$:
	\[\Dtan{\mu}_i = \qty(\pdv{\lambda_i} f(\lambda_i, \theta)) \Dtan{\lambda_i} + \qty(\pdv{\theta} f(\lambda_i, \theta)) \Dtan{\theta}.\]
	Designating
	$$D = \mqty(\dmat{{\pdv{\lambda_1} f(\lambda_1, \theta)}, \ddots, {\pdv{\lambda_n} f(\lambda_n, \theta)}}), \qquad E = \mqty(\dmat{{\pdv{\theta} f(\lambda_1, \theta)}, \ddots, {\pdv{\theta} f(\lambda_n, \theta)}}),$$
	we can write $\Dtan{M}$ as
	\[\Dtan{M} = \diag(\Dtan{\mu}) = D \left(I \circ (V^{-1} \Dtan{X} V)\right) + E \Dtan{\theta}.\]

	The pushforward of the final step is
	\begin{align*}
		\Dtan{\Omega}
		 & = \Dtan{V} M V^{-1} + V \Dtan{M} V^{-1} - V M V^{-1} \Dtan{V} V^{-1}   \\
		 & = \left( \Dtan{V} M + V \Dtan{M} - V M V^{-1} \Dtan{V} \right) V^{-1},
	\end{align*}
	where we have used the directional derivative of the matrix inverse from \cref{invdiff}.
	Substituting the derivatives from the previous steps, we find
	\begin{equation*}
		\Dtan{\Omega}
		= V \left(\left(W \circ \Dtan{\Lambda} \right) M -
		M \left(W \circ \Dtan{\Lambda} \right) +
		D \left(I \circ \Dtan{\Lambda} \right) +
		E \Dtan{\theta}
		\right) V^{-1},
	\end{equation*}
	where $\Dtan{\Lambda} = V^{-1} \Dtan{X} V$.

	For any diagonal matrices $J$ and $K$ and any square matrices $A$ and $B$ of the same size,
	\begin{equation*}
		(J (A \circ B) K)_{ij}
		= \sum_{kl} J_{ik} \delta_{ik} \qty(A_{kl} B_{kl}) K_{lj} \delta_{lj}
		= \qty(\sum_{kl} J_{ik} \delta_{ik} A_{kl} K_{lj} \delta_{lj}) B_{ij}
		= \qty( \qty(JAK) \circ B )_{ij}.
	\end{equation*}
	Therefore, we can simplify the expression for $\Dtan{\Omega}$ to
	\begin{equation*}
		\Dtan{\Omega}
		= V \left( (W M) \circ \Dtan{\Lambda} - (M W) \circ \Dtan{\Lambda} + D \circ \Dtan{\Lambda} + E \Dtan{\theta} \right) V^{-1}
		= V (P \circ \Dtan{\Lambda} + E \Dtan{\theta}) V^{-1},
	\end{equation*}
	where we call $P = (W M - M W + D)$ the pairwise difference quotient matrix.
	Computation of $P$ is discussed in the following section.

	In summary, in terms of $P$, the pushforward of $\Omega = F(X)$ is
	\begin{equation}\label{powseries_pf}
		\Dtan{\Omega} = V \left( P \circ \Dtan{\Lambda} + E \Dtan{\theta} \right) V^{-1},
	\end{equation}
	where, as defined above, $\Dtan{\Lambda} = V^{-1} \Dtan{X} V$.

	\section{The pairwise difference quotient matrix}\label{the-pairwise-difference-quotient-matrix}

	The entries of $P$ are
	\begin{align}
		P_{ij}
		 & = \left( \sum_k W_{ik} M_{kj} \delta_{kj} - \delta_{ik} M_{ik} W_{kj} \right) +
		\delta_{ij} D_{ij}  \nonumber                                                      \\
		 & = W_{ij} \mu_j - \mu_i W_{ij} + \delta_{ij} D_{ij}   \nonumber                  \\
		 & = W_{ij} (\mu_j - \mu_i) + \delta_{ij} D_{ij}  \nonumber                        \\
		 & = (1 - \delta_{ij}) \frac{f(\lambda_j) - f(\lambda_i)}{\lambda_j - \lambda_i} +
		\delta_{ij} f'(\lambda_i)   \nonumber                                              \\
		P_{ij}
		 & = \begin{cases}
			f'(\lambda_i) \quad                                             & \text{if } i = j   \\
			\frac{f(\lambda_j) - f(\lambda_i)}{\lambda_j - \lambda_i} \quad & \text{if } i \ne j
		\end{cases} \label{diffquotdef}.
	\end{align}
	We can unify the two cases in \cref{diffquotdef} by defining $P_{ij}$ in terms of a limit:
	\begin{equation} \label{diffquot}
		P_{ij} = \lim_{\epsilon \to 0}
		\frac{f(\lambda_j + \epsilon) - f(\lambda_i)}{\lambda_j - \lambda_i + \epsilon}.
	\end{equation}
	Expressed this way, we can see that the expression is defined for two important pathological cases.
	For degenerate matrices, where $\lambda_j = \lambda_i$ for some $i,j$, the corresponding entry is
	\begin{equation*}
		P_{ij} = \lim_{\epsilon \to 0}
		\frac{f(\lambda_j + \epsilon) - f(\lambda_j)}{\epsilon}
		= f'(\lambda_j) = f'(\lambda_i).
	\end{equation*}
	Diagonal and off-diagonal entries for singular matrices, where $\lambda_i=0$ for some $i$, require no special attention.

	However, for almost-degenerate matrices, which are more common due to numerical imprecision of the eigendecomposition, the naive computation of $P_{ij}$ can still suffer from roundoff error.
	We thus need a way to make the difference quotients more robust for $\lambda_j \approx \lambda_i$.
	One way to do this is to use the first-order Taylor expansion of $P_{ij}$ around $\Delta \lambda = \lambda_j - \lambda_i = 0$:
	\begin{equation}\label{diffquottaylor}
		P_{ij} = f'(\lambda_i) + \frac{\Delta \lambda}{2} f''(\lambda_i) + \mathcal{O}((\Delta \lambda)^2).
	\end{equation}
	However, this contains a second derivative, which some automatic differentiation systems may not be able to compute or which may be inefficient to compute.
	The Taylor series of the first derivative of $f$ is
	$$f'(\lambda_j) = f'(\lambda_i) + f''(\lambda_i) \Delta \lambda + \mathcal{O}((\Delta \lambda)^2).$$
	Rearranging, we find
	$$\Delta\lambda f''(\lambda_i) = f'(\lambda_j) - f'(\lambda_i) + \mathcal{O}((\Delta \lambda)^2).$$
	Inserting this expression into \cref{diffquottaylor}, we find
	\begin{align}
		P_{ij} & = f'(\lambda_i) + \frac{1}{2} \left( f'(\lambda_j) - f'(\lambda_i) \right) +
		\mathcal{O}((\Delta \lambda)^2) \nonumber                                                         \\
		       & = \frac{1}{2} \left( f'(\lambda_j) + f'(\lambda_i) \right) +
		\mathcal{O}((\Delta \lambda)^2) \nonumber                                                         \\
		       & \approx \frac{1}{2} \left( f'(\lambda_j) + f'(\lambda_i) \right). \label{diffquotapprox}
	\end{align}
	That is, when $(\Delta\lambda)^2 < \epsilon$ for some small $\epsilon$, we can approximate $P_{ij}$ with the average of the already-computed first derivatives of $f$, with a truncation error on the order of $\epsilon$, which could be taken for example to be machine epsilon $\epsilon_m$.

	\clearpage
	However, we use this approximation to counteract the roundoff error of the exact expression, which by a standard error analysis is on the order of $\frac{\epsilon_m}{\Delta\lambda}$, producing a total error on the order of
	\begin{equation} \label{diffquotapproxerr}
		\frac{\epsilon_m}{\Delta\lambda} + (\Delta\lambda)^2.
	\end{equation}

	Consequently, the optimal $\epsilon$ that minimizes the total error is on the order of $\epsilon_m^{2/3}$, where the total error is also on the same order.
	When using double-precision types, where machine epsilon is on the order of $10^{-16}$, the precision of $P_{ij}$ for almost-degenerate matrices at this threshold is then on the order of $10^{-10}$.
	For the $\exp$ and $\log$ functions, the empirical approximation of the total error agrees with this error analysis (\cref{fig:diffquot_error}).

	\begin{figure*}%[tbhp]
		\centering
		\includegraphics[width=8cm]{diffquot_error_exp.tikz}
		\includegraphics[width=8cm]{diffquot_error_log.tikz}
		\caption[Absolute actual and predicted error of the difference quotient approximation]{
			Absolute actual and predicted error of the difference quotient approximation for $\exp (0)$ (\emph{left}) and $\log (1)$ (\emph{right}).
			Actual error is defined as the difference of the difference quotient \cref{diffquot} and its approximation \cref{diffquotapprox}.
			Predicted error is defined by \cref{diffquotapproxerr}.
		}
		\label{fig:diffquot_error}
	\end{figure*}

	\section{Reverse-mode rule}\label{reverse-mode-rule-1}

	The fundamental identity of reverse-mode rules given in \cref{pbidentmat} here takes the form
	\begin{equation*}
		\Re \left( \tr \left( \hconj{\Dcot{\Omega}} \Dtan{\Omega} \right) \right) =
		\Re \left( \tr \left( \hconj{\Dcot{X}}      \Dtan{X}      \right) \right) +
		\Re \left( \conj{\Dcot{\theta}} \Dtan{\theta}  \right).
	\end{equation*}
	Substituting the pushforward into the left-hand side, we obtain
	\begin{align*}
		\Re \left( \tr \left( \hconj{\Dcot{\Omega}} \Dtan{\Omega} \right) \right)
		 & = \Re \left( \tr \left( \hconj{\Dcot{\Omega}} V \left( P \circ \Dtan{\Lambda} + E \Dtan{\theta} \right) V^{-1} \right) \right) \\
		 & = \Re \left( \tr \left( V^{-1} \hconj{\Dcot{\Omega}} V \left( P \circ \Dtan{\Lambda} + E \Dtan{\theta} \right) \right) \right) \\
		 & = \Re \left( \tr \left( \hconj{\Dcot{\Lambda}} \left( P \circ \Dtan{\Lambda} + E \Dtan{\theta} \right) \right) \right)         \\
		 & = \Re \left( \tr \left( \hconj{\Dcot{\Lambda}} \left( P \circ \Dtan{\Lambda} \right) \right) \right) +
		\Re \left( \tr \left( \hconj{\Dcot{\Lambda}} E \Dtan{\theta} \right) \right),
	\end{align*}
	where $\Dcot{\Lambda} = \hconj{V} \Dcot{\Omega} \invhconj{V}$.
	For symmetric $A$ and all other $B$ and $C$, we can write the following identity:
	\begin{equation*}
		\tr(C (A \circ B)) = \sum_{ijk} C_{ij} A_{jk} B_{jk} \delta_{ik}
		= \sum_{ij} C_{ij} A_{ji} B_{ji}
		= \tr((C \circ \trans{A}) B)
		= \tr((C \circ A) B).
	\end{equation*}
	Also, for diagonal $E$ and any $A$, we can write:
	\begin{equation*}
		\tr(A E) = \sum_{ijk} A_{ij} E_{jk} \delta_{jk} \delta_{ik}
		= \sum_i A_{ii} E_{ii}
		= \inner{\diag^{-1}(\hconj{A})}{\diag^{-1}(E)},
	\end{equation*}
	where $\diag^{-1}(\cdot)$ extracts the diagonal of a matrix as a vector.
	We can use these identities to bring $\Dtan{X}$ out of the Hadamard product and to simplify the $\Dtan{\theta}$ term:
	\begin{align*}
		\Re \left( \tr \left( \hconj{\Dcot{\Omega}} \Dtan{\Omega} \right) \right)
		 & = \Re \left( \tr \left(  \left( P \circ \hconj{\Dcot{\Lambda}} \right) V^{-1} \Dtan{X} V \right) \right) +
		\Re \left( \tr \left( \hconj{\Dcot{\Lambda}} E \right) \Dtan{\theta} \right)                                  \\
		 & = \Re \left( \tr \left( V \left( P \circ \hconj{\Dcot{\Lambda}} \right) V^{-1} \Dtan{X} \right) \right) +
		\Re \left( \inner{\diag^{-1}(\Dcot{\Lambda})}{\diag^{-1}(E)} \Dtan{\theta} \right).
	\end{align*}
	Comparing to the right-hand side of \cref{pbidentmat}, we solve for $\Dcot{X}$, giving the pullback function:
	\begin{align}
		\begin{split}\label{powseries_pb}
			\Dcot{X}       & = \invhconj{V} \left( \conj{P} \circ \Dcot{\Lambda} \right) \hconj{V} \\
			\Dcot{\theta}  & = \inner{\diag^{-1}(E)}{\diag^{-1}(\Dcot{\Lambda})},
		\end{split}
	\end{align}
	where, as given above, $\Dcot{\Lambda} = \hconj{V} \Dcot{\Omega} \invhconj{V}$.
	Strikingly, the forms of $\Dtan{\Omega}$ in \cref{powseries_pf} and $\hconj{\Dcot{X}}$ in \cref{powseries_pb} are quite similar for $\Dtan{\theta}=0$.

	\section{Power series functions of Hermitian matrices}\label{power-series-functions-of-hermitian-matrices}

	For a Hermitian matrix $X$, where $X = \hconj{X}$, the eigenvectors are unitary $U$ where $U \hconj{U} = \hconj{U} U = I$, and the eigenvalues are real.
	$\Dtan{X}$ is also Hermitian.
	The pushforwards and pullbacks then simplify:
	\begin{align}
		\Dtan{\Lambda} & = \hconj{U} \operatorname{herm} \left( \Dtan{X} \right) U \nonumber   \\
		\Dtan{\Omega}  & = U \left( P \circ \Dtan{\Lambda} + E \Dtan{\theta} \right) \hconj{U} \\
		\Dcot{\Lambda} & = \hconj{U} \Dcot{\Omega} U \nonumber                                 \\
		\begin{split}
			\Dcot{X}       & = \operatorname{herm} \left( U \left( \conj{P} \circ \Dcot{\Lambda} \right) \hconj{U} \right) \\
			\Dcot{\theta}  & = \inner{\diag^{-1}(E)}{\diag^{-1}(\Dcot{\Lambda})},
		\end{split}
	\end{align}
	where $\operatorname{herm}(A) = \frac{1}{2} \left( A + \hconj{A} \right)$ is an operator that makes a matrix Hermitian and is used above to enforce the necessary Hermitian constraints when a non-Hermitian tangent or cotangent vector is passed to the pushforward or pullback, respectively.

	If $f$ is a real function, that is, $f \colon \bbR \to \bbR$, then $\Omega = F(X)$ is always Hermitian with the same eigenvectors as $X$; consequently, $\Dcot{\Omega}$ is also Hermitian.
	Moreover, $\conj{P} = P$, and the forms of the pushforward and pullback are identical for $\Dtan{\theta}=0$.

	For $f \colon \bbR \to \bbC$, if $X$ is a real Hermitian matrix, then $U$ is real, and $\Omega = F(X)$ is a complex symmetric matrix:
	$$\Omega = U F(\Lambda) \hconj{U} = U \Re(F(\Lambda)) \hconj{U} + \im U \Im(F(\Lambda)) \hconj{U}$$

	If, on the other hand, $X$ is a complex Hermitian matrix, then $\Omega$ is neither symmetric nor Hermitian.
	In fact, it is the sum of a Hermitian and a skew-Hermitian matrix.
	However, because all complex square matrices can be written as such a sum, this tells us little about its structure.

	\clearpage
	\printbibliography[heading=subbibintoc]
\end{refsection}

\end{document}